Contents,6
Contributors,14
Preface,16
d=19
1 The Modern Mathematics of Deep Learning _Julius Berner, Philipp Grohs, Gitta Kutyniok and Philipp Petersen,1
	1.1 Introduction,1
		1.1.1 Notation,4
		1.1.2 Foundations of Learning Theory,5
		1.1.3 Do We Need a New Theory?,23
	1.2 Generalization of Large Neural Networks,31
		1.2.1 Kernel Regime,31
		1.2.2 Norm-Based Bounds and Margin Theory,33
		1.2.3 Optimization and Implicit Regularization,35
		1.2.4 Limits of Classical Theory and Double Descent,38
	1.3 The Role of Depth in the Expressivity of Neural Networks,41
		1.3.1 Approximation of Radial Functions,41
		1.3.2 Deep ReLU Networks,44
		1.3.3 Alternative Notions of Expressivity,47
	1.4 Deep Neural Networks Overcome the Curse of Dimensionality,49
		1.4.1 Manifold Assumption,49
		1.4.2 Random Sampling,51
		1.4.3 PDE Assumption,53
	1.5 Optimization of Deep Neural Networks,57
		1.5.1 Loss Landscape Analysis,57
		1.5.2 Lazy Training and Provable Convergence of Stochastic Gradient Descent,61
	1.6 Tangible Effects of Special Architectures,65
		1.6.1 Convolutional Neural Networks,66
		1.6.2 Residual Neural Networks,68
		1.6.3 Framelets and U-Nets,70
		1.6.4 Batch Normalization,73
		1.6.5 Sparse Neural Networks and Pruning,75
		1.6.6 Recurrent Neural Networks,76
	1.7 Describing the Features that a Deep Neural Network Learns,78
		1.7.1 Invariances and the Scattering Transform,78
		1.7.2 Hierarchical Sparse Representations,79
	1.8 Effectiveness in Natural Sciences,81
		1.8.1 Deep Neural Networks Meet Inverse Problems,82
		1.8.2 PDE-Based Models,84
2 Generalization in Deep Learning _K. Kawaguchi, Y. Bengio, and L. Kaelbling,112
	2.1 Introduction,112
	2.2 Background,113
	2.3 Rethinking Generalization,116
		2.3.1 Consistency of Theory,118
		2.3.2 Differences in Assumptions and Problem Settings,119
		2.3.3 Practical Role of Generalization Theory,121
	2.4 Generalization Bounds via Validation,121
	2.5 Direct Analyses of Neural Networks,122
		2.5.1 Model Description via Deep Paths,123
		2.5.2 Theoretical Insights via Tight Theory for Every Pair (P, S),125
		2.5.3 Probabilistic Bounds over Random Datasets,127
		2.5.4 Probabilistic Bound for 0–1 Loss with Multi-Labels,130
	2.6 Discussions and Open Problems,131
	Appendix A Additional Discussions,133
		A1 Simple Regularization Algorithm,133
		A2 Relationship to Other Fields,135
		A3 SGD Chooses Direction in Terms of w̄,135
		A4 Simple Implementation of Two-Phase Training Procedure,136
		A5 On Proposition 2.3,136
		A6 On Extensions,137
	Appendix B Experimental Details,137
	Appendix C Proofs,138
		C1 Proof of Theorem 2.1,139
		C2 Proof of Corollary 2.2,139
		C3 Proof of Theorem 2.7,140
		C4 Proof of Theorem 2.9,141
		C5 Proof of Theorem 2.10,142
		C6 Proof of Proposition 2.5,143
3 Expressivity of Deep Neural Networks _Ingo Gühring, Mones Raslan, and Gitta Kutyniok,149
	3.1 Introduction,149
		3.1.1 Neural Networks,151
		3.1.2 Goal and Outline of this Chapter,154
		3.1.3 Notation,154
	3.2 Shallow Neural Networks,155
		3.2.1 Universality of Shallow Neural Networks,156
		3.2.2 Lower Complexity Bounds,159
		3.2.3 Upper Complexity Bounds,160
	3.3 Universality of Deep Neural Networks,161
	3.4 Approximation of Classes of Smooth Functions,163
	3.5 Approximation of Piecewise Smooth Functions,167
	3.6 Assuming More Structure,172
		3.6.1 Hierachical Structure,172
		3.6.2 Assumptions on the Data Manifold,174
		3.6.3 Expressivity of Deep Neural Networks for Solutions of PDEs,175
	3.7 Deep Versus Shallow Neural Networks,177
	3.8 Special Neural Network Architectures and Activation Functions,180
		3.8.1 Convolutional Neural Networks,180
		3.8.2 Residual Neural Networks,184
		3.8.3 Recurrent Neural Networks,185
4 Optimization Landscape of Neural Networks _René Vidal, Zhihui Zhu, and Benjamin D. Haeffele,200
	4.1 Introduction,201
	4.2 Basics of Statistical Learning,205
	4.3 Optimization Landscape of Linear Networks,206
		4.3.1 Single-Hidden-Layer Linear Networks with Squared Loss and Fixed Size Regularization,207
		4.3.2 Deep Linear Networks with Squared Loss,212
	4.4 Optimization Landscape of Nonlinear Networks,214
		4.4.1 Motivating Example,215
		4.4.2 Positively Homogeneous Networks,221
	4.5 Conclusions,225
5 Explaining the Decisions of Convolutional and Recurrent Neural Networks _Wojciech Samek, Leila Arras, Ahmed Osman, Grégoire Montavon, Klaus-Robert Müller,229
	5.1 Introduction,229
	5.2 Why Explainability?,231
		5.2.1 Practical Advantages of Explainability,231
		5.2.2 Social and Legal Role of Explainability,232
		5.2.3 Theoretical Insights Through Explainability,232
	5.3 From Explaining Linear Models to General Model Explainability,233
		5.3.1 Explainability of Linear Models,233
		5.3.2 Generalizing Explainability to Nonlinear Models,235
		5.3.3 Short Survey on Explanation Methods,236
	5.4 Layer-Wise Relevance Propagation,238
		5.4.1 LRP in Convolutional Neural Networks,239
		5.4.2 Theoretical Interpretation of the LRP Redistribution Process,242
		5.4.3 Extending LRP to LSTM Networks,248
	5.5 Explaining a Visual Question Answering Model,251
	5.6 Discussion,258
6 Stochastic Feedforward Neural Networks: Universal Approximation _Thomas Merkh and Guido Montúfar,267
	6.1 Introduction,268
	6.2 Overview of Previous Works and Results,271
	6.3 Markov Kernels and Stochastic Networks,273
		6.3.1 Binary Probability Distributions and Markov Kernels,273
		6.3.2 Stochastic Feedforward Networks,274
	6.4 Results for Shallow Networks,276
		6.4.1 Fixed Weights in the Output Layer,277
		6.4.2 Trainable Weights in the Output Layer,278
	6.5 Proofs for Shallow Networks,278
		6.5.1 Fixed Weights in the Output Layer,279
		6.5.2 Trainable Weights in the Second Layer,283
		6.5.3 Discussion of the Proofs for Shallow Networks,285
	6.6 Results for Deep Networks,286
		6.6.1 Parameter Count,288
		6.6.2 Approximation with Finite Weights and Biases,288
	6.7 Proofs for Deep Networks,289
		6.7.1 Notation,289
		6.7.2 Probability Mass Sharing,290
		6.7.3 Universal Approximation,293
		6.7.4 Error Analysis for Finite Weights and Biases,296
		6.7.5 Discussion of the Proofs for Deep Networks,298
	6.8 Lower Bounds for Shallow and Deep Networks,299
		6.8.1 Parameter Counting Lower Bounds,299
		6.8.2 Minimum Width,301
	6.9 A Numerical Example,302
	6.10 Conclusion,306
	6.11 Open Problems,307
7 Deep Learning as Sparsity-Enforcing Algorithms _A. Aberdam and J. Sulam,314
	7.1 Introduction,314
	7.2 Related Work,316
	7.3 Background,317
	7.4 Multilayer Sparse Coding,320
		7.4.1 ML–SC Pursuit and the Forward Pass,321
		7.4.2 ML–SC: A Projection Approach,323
	7.5 The Holistic Way,324
	7.6 Multilayer Iterative Shrinkage Algorithms,327
		7.6.1 Towards Principled Recurrent Neural Networks,329
	7.7 Final Remarks and Outlook,332
8 The Scattering Transform _Joan Bruna,338
	8.1 Introduction,338
	8.2 Geometric Stability,339
		8.2.1 Euclidean Geometric Stability,340
		8.2.2 Representations with Euclidean Geometric Stability,341
		8.2.3 Non-Euclidean Geometric Stability,342
		8.2.4 Examples,343
	8.3 Scattering on the Translation Group,346
		8.3.1 Windowed Scattering Transform,346
		8.3.2 Scattering Metric and Energy Conservation,349
		8.3.3 Local Translation Invariance and Lipschitz Continuity with Respect to Deformations,351
		8.3.4 Algorithms,354
		8.3.5 Empirical Analysis of Scattering Properties,357
		8.3.6 Scattering in Modern Computer Vision,362
	8.4 Scattering Representations of Stochastic Processes,363
		8.4.1 Expected Scattering,363
		8.4.2 Analysis of Stationary Textures with Scattering,367
		8.4.3 Multifractal Analysis with Scattering Moments,369
	8.5 Non-Euclidean Scattering,371
		8.5.1 Joint versus Separable Scattering,372
		8.5.2 Scattering on Global Symmetry Groups,372
		8.5.3 Graph Scattering,375
		8.5.4 Manifold Scattering,383
	8.6 Generative Modeling with Scattering,384
		8.6.1 Sufficient Statistics,384
		8.6.2 Microcanonical Scattering Models,385
		8.6.3 Gradient Descent Scattering Reconstruction,387
		8.6.4 Regularising Inverse Problems with Scattering,389
		8.6.5 Texture Synthesis with Microcanonical Scattering,391
	8.7 Final Remarks,393
9 Deep Generative Models and Inverse Problems _Alexandros G. Dimakis,400
	9.1 Introduction,400
	9.2 How to Tame High Dimensions,401
		9.2.1 Sparsity,401
		9.2.2 Conditional Independence,402
		9.2.3 Deep Generative Models,403
		9.2.4 GANs and VAEs,404
		9.2.5 Invertible Generative Models,405
		9.2.6 Untrained Generative Models,405
	9.3 Linear Inverse Problems Using Deep Generative Models,406
		9.3.1 Reconstruction from Gaussian Measurements,407
		9.3.2 Optimization Challenges,409
		9.3.3 Extending the Range of the Generator,410
		9.3.4 Non-Linear Inverse Problems,410
		9.3.5 Inverse Problems with Untrained Generative Priors,412
	9.4 Supervised Methods for Inverse Problems,414
10 Dynamical Systems and Optimal Control Approach to Deep Learning _Weinan E, Jiequn Han, and Qianxiao Li,422
	10.1 Introduction,422
		10.1.1 The Problem of Supervised Learning,423
	10.2 ODE Formulation,424
	10.3 Mean-Field Optimal Control and Pontryagin’s Maximum Principle,425
		10.3.1 Pontryagin’s Maximum Principle,426
	10.4 Method of Successive Approximations,428
		10.4.1 Extended Pontryagin Maximum Principle,428
		10.4.2 The Basic Method of Successive Approximation,428
		10.4.3 Extended Method of Successive Approximation,431
		10.4.4 Discrete PMP and Discrete MSA,433
	10.5 Future Work,435
11 Bridging Many-Body Quantum Physics and Deep Learning via Tensor Networks _Yoav Levine, Or Sharir, Nadav Cohen and Amnon Shashua,439
	11.1 Introduction,440
	11.2 Preliminaries – Many-Body Quantum Physics,442
		11.2.1 The Many-Body Quantum Wave Function,443
		11.2.2 Quantum Entanglement Measures,444
		11.2.3 Tensor Networks,447
	11.3 Quantum Wave Functions and Deep Learning Architectures,450
		11.3.1 Convolutional and Recurrent Networks as Wave Functions,450
		11.3.2 Tensor Network Representations of Convolutional and Recurrent Networks,453
	11.4 Deep Learning Architecture Design via Entanglement Measures,453
		11.4.1 Dependencies via Entanglement Measures,454
		11.4.2 Quantum-Physics-Inspired Control of Inductive Bias,456
	11.5 Power of Deep Learning for Wave Function Representations,460
		11.5.1 Entanglement Scaling of Deep Recurrent Networks,461
		11.5.2 Entanglement Scaling of Overlapping Convolutional Networks,463
	11.6 Discussion,467
